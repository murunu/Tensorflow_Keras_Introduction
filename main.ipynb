{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "This notebook is made for training and making predictions for our golf ball prediction project.  \n",
    "This is a project where we want to get a lot of prediction data out of a video where a person hits a golf ball.\n",
    "\n",
    "\n",
    "For this part of the project we will be matching the data we get from the videos with the data from simulations we ran.  \n",
    "To do this we are going to use Tensorflow, more specifically a deep learning API running on top of Tensorflow called `Keras`.  \n",
    "\n",
    "The document will be Generic, this means you can train any model if you input a JSON file.\n",
    "\n",
    "See [Get dataset](#2-get-dataset) for more information.\n",
    "\n",
    "\n",
    "At the end of this notebook we will have a trained model and we will be able to make a prediction of how hard you hit a golf ball (and some other data) on all of the video data we put in to the model.\n",
    "\n",
    "**Make sure to use an environment with [Tensorflow](https://www.tensorflow.org/install) and [MatPlotLib](https://matplotlib.org/stable/users/installing/index.html) installed.**\n",
    "\n",
    "If you are just starting out using Python make sure to read about [Virtual environments](https://virtualenv.pypa.io/en/latest/).\n",
    "\n",
    "Using Google Colab you can run all of this code, simply by pressing the play button when hovering over a code block. Or to run everything in this document, go to Runtime on the top navbar --> Run everything.\n",
    "See https://colab.research.google.com/notebooks/basic_features_overview.ipynb for more information about Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Table of contents\n",
    "1. Imports\n",
    "1. Get dataset\n",
    "1. Make or import the model\n",
    "1. Train the model\n",
    "1. Get plot from history\n",
    "1. Save the model\n",
    "1. Predict something\n",
    "2. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run the following lines of code to install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (3.17.2)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: keras>=2.4.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.2 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (1.22.3)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: wheel>=0.35 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (0.15.0)\n",
      "Collecting flatbuffers~=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.1.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.6.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2022.5.18.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.2.0)\n",
      "Building wheels for collected packages: clang\n",
      "  Building wheel for clang (setup.py): started\n",
      "  Building wheel for clang (setup.py): finished with status 'done'\n",
      "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30701 sha256=aacb0a8d51c3020126e887f2ae2691bcedcd2851289ad3ba55973f69f0bd52ae\n",
      "  Stored in directory: c:\\users\\moren\\appdata\\local\\pip\\cache\\wheels\\3a\\ce\\7a\\27094f689461801c934296d07078773603663dfcaca63bb064\n",
      "Successfully built clang\n",
      "Installing collected packages: google-auth, flatbuffers, clang\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.6.0\n",
      "    Uninstalling google-auth-2.6.0:\n",
      "      Successfully uninstalled google-auth-2.6.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 20210226132247\n",
      "    Uninstalling flatbuffers-20210226132247:\n",
      "      Successfully uninstalled flatbuffers-20210226132247\n",
      "Successfully installed clang-5.0 flatbuffers-1.12 google-auth-1.35.0\n",
      "Requirement already satisfied: matplotlib in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (3.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from matplotlib) (1.22.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\moren\\.conda\\envs\\keras\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get dataset\n",
    "\n",
    "This function will get data from the folder `datasets` and convert every json file to [Tensorflow Tensors](https://www.tensorflow.org/api_docs/python/tf/Tensor).  \n",
    "We need these Tensors because that is an expected input for training the model later.\n",
    "\n",
    "Because the json array we get is a nested array we need to `flatten` the data first, we do this using a [Tensorflow flatten function](https://www.tensorflow.org/api_docs/python/tf/nest/flatten)  \n",
    "After the input is flattened you can simply [Convert to tensor](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor) and use this tensor as an input or output when training the model.\n",
    "\n",
    "At first we will be loading some random data in case there is no datasets directory.\n",
    "\n",
    "To use your own data, create a `datasets` directory and put some json files in it. The block of code below will automatically find these files and use them to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some random data\n",
    "random_data = \"\"\"\n",
    "{\n",
    "    \"inputs\": [\n",
    "      {\n",
    "        \"time\": 1,\n",
    "        \"coordinatesInput\": {\n",
    "          \"x1\": 1,\n",
    "          \"x2\": 2,\n",
    "          \"y1\": 1,\n",
    "          \"y2\": 2\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"time\": 2,\n",
    "        \"coordinatesInput\": {\n",
    "          \"x1\": 2,\n",
    "          \"x2\": 3,\n",
    "          \"y1\": 2,\n",
    "          \"y2\": 3\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"time\": 3,\n",
    "        \"coordinatesInput\": {\n",
    "          \"x1\": 3,\n",
    "          \"x2\": 4,\n",
    "          \"y1\": 3,\n",
    "          \"y2\": 4\n",
    "        }\n",
    "      },\n",
    "      {\n",
    "        \"time\": 4,\n",
    "        \"coordinatesInput\": {\n",
    "          \"x1\": 4,\n",
    "          \"x2\": 5,\n",
    "          \"y1\": 4,\n",
    "          \"y2\": 5\n",
    "        }\n",
    "      }\n",
    "    ],\n",
    "    \"outputs\": {\n",
    "      \"speed\": 45,\n",
    "      \"angle\": 45,\n",
    "      \"spinAxis\": 45,\n",
    "      \"distance\": 45,\n",
    "      \"rpm\": 1000,\n",
    "      \"resultData\": [\n",
    "        {\n",
    "          \"time\": 1,\n",
    "          \"coordinatesOutput\": {\n",
    "            \"x\": 1,\n",
    "            \"y\": 1,\n",
    "            \"z\": 1\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"time\": 2,\n",
    "          \"coordinatesOutput\": {\n",
    "            \"x\": 2,\n",
    "            \"y\": 2,\n",
    "            \"z\": 2\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"time\": 3,\n",
    "          \"coordinatesOutput\": {\n",
    "            \"x\": 3,\n",
    "            \"y\": 3,\n",
    "            \"z\": 3\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"time\": 4,\n",
    "          \"coordinatesOutput\": {\n",
    "            \"x\": 4,\n",
    "            \"y\": 4,\n",
    "            \"z\": 4\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No files found in datasets directory using random data defined before\n",
      "input: [{'time': 1, 'coordinatesInput': {'x1': 1, 'x2': 2, 'y1': 1, 'y2': 2}}, {'time': 2, 'coordinatesInput': {'x1': 2, 'x2': 3, 'y1': 2, 'y2': 3}}, {'time': 3, 'coordinatesInput': {'x1': 3, 'x2': 4, 'y1': 3, 'y2': 4}}, {'time': 4, 'coordinatesInput': {'x1': 4, 'x2': 5, 'y1': 4, 'y2': 5}}] \n",
      "\n",
      "flattened input: [1, 2, 1, 2, 1, 2, 3, 2, 3, 2, 3, 4, 3, 4, 3, 4, 5, 4, 5, 4] \n",
      "\n",
      "tensor input: tf.Tensor([[1 2 1 2 1 2 3 2 3 2 3 4 3 4 3 4 5 4 5 4]], shape=(1, 20), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "directory = os.path.join(os.getcwd(), \"datasets\")\n",
    "all_input_files = np.array([])\n",
    "input = []\n",
    "output = []\n",
    "\n",
    "if os.path.isdir(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            all_input_files = np.append(all_input_files, filename)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    for i in range(len(all_input_files)):\n",
    "        x = json.load(open(os.path.join(directory, all_input_files[i])))\n",
    "\n",
    "        print(f\"input {i}:\", x[\"inputs\"], \"\\n\")\n",
    "        print(f\"flattened input {i}:\", tf.nest.flatten(x[\"inputs\"]), \"\\n\")\n",
    "\n",
    "        input.append(tf.nest.flatten(x[\"inputs\"]))\n",
    "        output.append(tf.nest.flatten(x[\"outputs\"]))\n",
    "else:\n",
    "    print(\"No files found in datasets directory using random data defined before\")\n",
    "    x = json.loads(random_data)\n",
    "    print(\"input:\", x[\"inputs\"], \"\\n\")\n",
    "    print(\"flattened input:\", tf.nest.flatten(x[\"inputs\"]), \"\\n\")\n",
    "\n",
    "    input.append(tf.nest.flatten(x[\"inputs\"]))\n",
    "    output.append(tf.nest.flatten(x[\"outputs\"]))\n",
    "\n",
    "train_ds_input = tf.convert_to_tensor(input)\n",
    "train_ds_output = tf.convert_to_tensor(output)\n",
    "\n",
    "print(\"tensor input:\", train_ds_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make or import the model\n",
    "This function will either:\n",
    "* Import a model if it exists in the models folder\n",
    "* Create a new model\n",
    "\n",
    "For more information about a model read about it here: [ts.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(os.path.join(os.getcwd(), \"./models/model.h5\")):\n",
    "    model = tf.keras.models.load_model(os.path.join(os.getcwd(), \"./models/model.h5\"))\n",
    "    print(\"Model loaded\")\n",
    "else:\n",
    "    inputs = tf.keras.Input(shape=(20,), name=\"inputs\")\n",
    "    x = tf.keras.layers.Dense(20, activation=tf.nn.relu)(inputs)\n",
    "    outputs = tf.keras.layers.Dense(21, activation=tf.nn.softmax)(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    print(\"Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make callbacks\n",
    "First we need to make callbacks.\n",
    "\n",
    "There is several reasons you might want to make callbacks, we will only use it to do the following things:\n",
    "We log the training process in the \"logs\" folder.  \n",
    "We also make checkpoints everytime the accuracy increases so we can load that later if something happens like the machine crashes.\n",
    "\n",
    "For more information about callbacks read: [tf.keras.callbacks.Callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"./models/weights-improvement-{epoch:02d}.hdf5\"\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Here we get to actually train the model.\n",
    "\n",
    "We use the `train_ds_input` and `train_ds_output` we made in an earlier code block. This is the training data.  \n",
    "For now there is 100 `epochs` and 100 `steps_per_epoch` this can be changed anytime. For now I will leave it like this since there is only 2 inputs and it will not train anyway.\n",
    "\n",
    "While training a dataset you don't want to many `epochs` and `steps_per_epoch`, this is because of something called `overfit`. `overfit` occurs when you train your model too long, because of this the model we are training might be too good for the input data.\n",
    "In simple terms you might say a student comes to a test already knowing the answers. Because the student already knows the answers he does not have to think about it. This will also mean that if the student would get another question he might not know the answer.\n",
    "\n",
    "It is also not good to do to little `epochs` and `steps_per_epoch`, if you do to little something might happen called `underfit`. This means our student has not learned enough for the test and will not know the answer.\n",
    "\n",
    "For more information about `overfit` and `underfit` see: https://www.tensorflow.org/tutorials/keras/overfit_and_underfit.\n",
    "\n",
    "We also need to set the callbacks to `my_callbacks` we made in the above code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: -1443.4636 - accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: accuracy improved from -inf to 1.00000, saving model to ./models\\weights-improvement-01.hdf5\n",
      "Epoch 2/100\n",
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 10000 batches). You may need to use the repeat() function when building your dataset.\n",
      "100/100 [==============================] - 0s 120us/step - loss: -1443.4636 - accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: accuracy did not improve from 1.00000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds_input, train_ds_output, epochs=100, steps_per_epoch=100, batch_size=8000, callbacks=my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get plot from history\n",
    "From the training process we get all sorts of data we can use to plot to visualize how well the model has trained.\n",
    "\n",
    "In this function we make a plot using matplotlib using the `history` from the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_acc']) # TODO: add validation accuracy\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the model\n",
    "The model is done training and we can save it now, this way we dont have to use checkpoints each time.  \n",
    "This model is reusable for next time we want to maybe train it better or make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save(\"./models/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Predict something\n",
    "Here we define a functions that uses our trained model to make a prediction on the data (`input`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input):\n",
    "    # Make array of input\n",
    "    data = []\n",
    "    data.append(tf.nest.flatten(input))\n",
    "    \n",
    "    # Load the model\n",
    "    model = tf.keras.models.load_model(os.path.join(os.getcwd(), \"./models/model.h5\"))\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "    # Make a prediction\n",
    "    prediction = model.predict(tf.convert_to_tensor(data))\n",
    "    prediction = prediction.tolist()\n",
    "    return tf.nest.pack_sequence_as(get_structure(), prediction[0])\n",
    "\n",
    "def get_structure():\n",
    "    # Check for json files in datasets directory and return outputs for structure\n",
    "    directory = os.path.join(os.getcwd(), \"datasets\")\n",
    "    \n",
    "    if os.path.isdir(directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".json\"):\n",
    "                x = json.load(open(os.path.join(directory, filename)))\n",
    "                return x[\"outputs\"]\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        x = json.loads(random_data)\n",
    "        return x[\"outputs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we can use this function to make a prediction.  \n",
    "The prediction itself is not very readable since it is still a flattened array.  \n",
    "To make the prediction a bit more readable we can use a Tensorflow function [tf.nest.pack_sequence_as()](https://www.tensorflow.org/api_docs/python/tf/nest/pack_sequence_as).  \n",
    "In this function we use the original structure for the output.\n",
    "\n",
    "Right here we only go through all the files in the datasets directory and make a prediction. For other predictions just **import predict** and use the `predict` function to make a prediction of a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025FEB30D9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "{   'angle': 2.4020595415239652e-17,\n",
      "    'distance': 1.972563144726422e-13,\n",
      "    'resultData': [   {   'coordinatesOutput': {   'x': 2.5846332375893726e-25,\n",
      "                                                   'y': 2.4166315855871118e-21,\n",
      "                                                   'z': 5.611667436176224e-26},\n",
      "                          'time': 1.3249467950376927e-22},\n",
      "                      {   'coordinatesOutput': {   'x': 3.854218014270142e-20,\n",
      "                                                   'y': 2.3565539269749607e-23,\n",
      "                                                   'z': 2.1264957643018282e-21},\n",
      "                          'time': 2.5439379130332905e-20},\n",
      "                      {   'coordinatesOutput': {   'x': 3.798582779432954e-17,\n",
      "                                                   'y': 2.660290461261836e-23,\n",
      "                                                   'z': 1.0018622154881997e-17},\n",
      "                          'time': 4.651755640220052e-19},\n",
      "                      {   'coordinatesOutput': {   'x': 5.244968895059052e-21,\n",
      "                                                   'y': 9.029345575131852e-18,\n",
      "                                                   'z': 8.873015373459309e-21},\n",
      "                          'time': 2.948732276328361e-22}],\n",
      "    'rpm': 1.0,\n",
      "    'speed': 1.4559296752889874e-15,\n",
      "    'spinAxis': 5.8430619690559224e-15}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "my_printer = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "directory = os.path.join(os.getcwd(), \"datasets\")\n",
    "if os.path.isdir(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            x = json.load(open(os.path.join(directory, filename)))\n",
    "            my_printer.pprint(predict(x[\"inputs\"]))\n",
    "        else:\n",
    "            continue\n",
    "else:\n",
    "    x = json.loads(random_data)\n",
    "    my_printer.pprint(predict(x[\"inputs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "In this notebook you have learned about the absolute basics of [Tensorflow Keras](https://keras.io/).\n",
    "We have setup and trained a very simple model to predict data for calculating the trajectory of a golf ball.\n",
    "\n",
    "There is alot of datapacks you can find on the internet, you can try and train your own model using [Tensorflow Keras](https://keras.io/guides/).\n",
    "\n",
    "### Further read\n",
    "On the [Tensorflow Keras](https://keras.io/guides/) website there is more [guides](https://keras.io/guides/) for more advanced readings about using Keras.\n",
    "[Tensorflow](https://www.tensorflow.org/) has more API's then just Keras. If you want to go into `Tensorflow object detection` you can go to their site and find [Object detection](https://www.tensorflow.org/lite/examples/object_detection/overview)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Keras')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "c09bc7f71d981f55b3347e1865413f6f28fb8472dda7a4b51be01af2ad4d016d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
